# -------------------------- BOSTON HOUSING LINEAR REFRESSION DNN -----------------------------------------------
 
import numpy as np
import pandas as pd
import tensorflow as tf
from keras.datasets import boston_housing
from keras.models import Sequential
from keras.layers import Dense
from sklearn.metrics import mean_squared_error
 
# Load the Boston Housing dataset
(X_train, y_train), (X_test, y_test) = boston_housing.load_data()
 
# Dataset description
print("Dataset Description:")
print("Training samples:", X_train.shape[0])
print("Testing samples:", X_test.shape[0])
print("Number of features:", X_train.shape[1])
 
# Convert the data into a pandas DataFrame
column_names = ['CRIM', 'ZN', 'INDUS', 'CHAS', 'NOX', 'RM', 'AGE', 'DIS', 'RAD', 'TAX', 'PTRATIO', 'B', 'LSTAT', 'MEDV']
df = pd.DataFrame(np.concatenate((X_train, y_train.reshape(-1, 1)), axis=1), columns=column_names)
 
# Dataset head and tail
print("Dataset Head:")
print(df.head())
 
print("Dataset Tail:")
print(df.tail())
 
# Preprocess the data
mean = X_train.mean(axis=0)
std = X_train.std(axis=0)
X_train = (X_train - mean) / std
X_test = (X_test - mean) / std
 
# Create the neural network model
model = Sequential()
model.add(Dense(64, activation='relu', input_shape=(X_train.shape[1],)))
model.add(Dense(64, activation='relu'))
model.add(Dense(1))
model.compile(optimizer='adam', loss='mean_squared_error')
model.summary()
 
# Train the model
history = model.fit(X_train, y_train, epochs=100, batch_size=32, validation_split=0.2, verbose=0)
 
# Evaluate the model
y_pred = model.predict(X_test)
mse = mean_squared_error(y_test, y_pred)
print("Mean Squared Error:", mse)
 
#--------------------------------OCR MULTICLASS CLASSIFICATION USING DNN-----------------------------------------------
import numpy as np
import matplotlib.pyplot as plt
import tensorflow as tf
from keras.datasets import mnist
from keras.models import Sequential
from keras.layers import Dense
 
 
# Load the MNIST dataset
(X_train, y_train), (X_test, y_test) = mnist.load_data()
 
# Dataset description
print("Dataset Description:")
print("Training samples:", X_train.shape[0])
print("Testing samples:", X_test.shape[0])
print("Image shape:", X_train.shape[1:])
 
# Dataset visualization
fig, axes = plt.subplots(2, 5, figsize=(10, 5))
axes = axes.flatten()
 
for i, ax in enumerate(axes):
    ax.imshow(X_train[i], cmap='gray')
    ax.axis('off')
    ax.set_title(f"Label: {y_train[i]}")
 
plt.tight_layout()
plt.show()
 
# Print dataset head and tail
print("Dataset Head:")
print(X_train[:5])
print(y_train[:5])
 
print("Dataset Tail:")
print(X_train[-5:])
print(y_train[-5:])
 
# Preprocess the data
X_train = X_train.reshape(-1, 28 * 28) / 255.0
X_test = X_test.reshape(-1, 28 * 28) / 255.0
 
# Create the neural network model
model = Sequential()
model.add(Dense(256, activation='relu', input_shape=(28 * 28,)))
model.add(Dense(128, activation='relu'))
model.add(Dense(10, activation='softmax'))
model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])
model.summary()
 
# Train the model
history = model.fit(X_train, y_train, epochs=10, batch_size=32, validation_split=0.1)
 
 
 
# Plot the training and validation loss
plt.figure(figsize=(8, 5))
plt.plot(history.history['loss'], label='Training Loss')
plt.plot(history.history['val_loss'], label='Validation Loss')
plt.xlabel('Epoch')
plt.ylabel('Loss')
plt.title('Training and Validation Loss')
plt.legend()
plt.show()
 
# Plot the training and validation accuracy
plt.figure(figsize=(8, 5))
plt.plot(history.history['accuracy'], label='Training Accuracy')
plt.plot(history.history['val_accuracy'], label='Validation Accuracy')
plt.xlabel('Epoch')
plt.ylabel('Accuracy')
plt.title('Training and Validation Accuracy')
plt.legend()
plt.show()
 
# Make predictions on the test set
y_pred = model.predict(X_test)
 
# Evaluate the model
loss, accuracy = model.evaluate(X_test, y_test)
print("Test Loss:", loss)
print("Test Accuracy:", accuracy)
 
# Visualization of predictions
fig, axes = plt.subplots(2, 2, figsize=(15, 15))
axes = axes.flatten()
 
for i, ax in enumerate(axes):
    ax.imshow(X_test[i].reshape(28, 28), cmap='gray')
    ax.axis('off')
    ax.set_title(f"Pred: {y_pred[i]}, True: {y_test[i]}")
 
plt.tight_layout()
plt.show()
 
# -------------------------------------------------------BINARY CLASSIFICATION POSITIVE NEGATIVE REVIEW IMDB WITH VISUALIZATION-------------------------------------
import numpy as np
import matplotlib.pyplot as plt
import tensorflow as tf
from keras.datasets import imdb
from keras.utils import pad_sequences
from keras.models import Sequential
from keras.layers import Dense
 
 
# Load the IMDB dataset
(X_train, y_train), (X_test, y_test) = imdb.load_data(num_words=10000)
 
# Preprocess the data
max_len = 200  # Maximum sequence length
X_train = pad_sequences(X_train, maxlen=max_len)
X_test = pad_sequences(X_test, maxlen=max_len)
 
# Display a few samples from the dataset
word_index = imdb.get_word_index()
reverse_word_index = dict([(value, key) for (key, value) in word_index.items()])
 
def decode_review(review):
    return ' '.join([reverse_word_index.get(i - 3,"-" ) for i in review])
 
print("Sample Positive Review:")
print(decode_review(X_train[6]))
print("Sample Negative Review:")
print(decode_review(X_train[5]))
 
# Create the deep neural network model
model = Sequential()
model.add(Dense(256, activation='relu', input_shape=(max_len,)))
model.add(Dense(128, activation='relu'))
model.add(Dense(128, activation='relu'))
model.add(Dense(1, activation='sigmoid'))
model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])
model.summary()
 
# Train the model
history = model.fit(X_train, y_train, epochs=10, batch_size=32, validation_split=0.1)
 
# Make predictions on the test set
y_pred = model.predict(X_test)
 
# Calculate the accuracy of the model
score = model.evaluate(X_test,y_test,verbose=0)
print("Test Loss", score[0])
print("Test Accuracy", score[1])
 
# Plot the training and validation accuracy
plt.figure(figsize=(8, 5))
plt.plot(history.history['accuracy'], label='Training Accuracy')
plt.plot(history.history['val_accuracy'], label='Validation Accuracy')
plt.xlabel('Epoch')
plt.ylabel('Accuracy')
plt.title('Training and Validation Accuracy')
plt.legend()
plt.show()
 
# Visualize a few sample predictions
fig, axes = plt.subplots(2, 5, figsize=(12, 6))
axes = axes.flatten()
 
for i, ax in enumerate(axes):
    index = np.random.randint(0, len(X_test))
    review = decode_review(X_test[index])
    pred_label = 'Positive' if y_pred[index] == 1 else 'Negative'
    true_label = 'Positive' if y_test[index] == 1 else 'Negative'
    color = 'green' if pred_label == true_label else 'red'
    ax.text(0.5, 0.5, f"Pred: {pred_label}\nTrue: {true_label}", color=color, fontsize=12, ha='center')
    ax.axis('off')
 
plt.tight_layout()
plt.show()
 
# -----------------------------------------------------------FASHION MNIST----------------------------------------------------------------------
 
import numpy as np
import matplotlib.pyplot as plt
import tensorflow as tf
from keras.datasets import fashion_mnist
from keras.models import Sequential
from keras.layers import Dense
from sklearn.metrics import accuracy_score
 
# Load the MNIST Fashion dataset
(X_train, y_train), (X_test, y_test) = fashion_mnist.load_data()
 
# Dataset description
print("Dataset Description:")
print("Training samples:", X_train.shape[0])
print("Testing samples:", X_test.shape[0])
print("Image shape:", X_train.shape[1:])
 
# Map label indices to class names
class_names = ['T-shirt/top', 'Trouser', 'Pullover', 'Dress', 'Coat',
               'Sandal', 'Shirt', 'Sneaker', 'Bag', 'Ankle boot']
 
# Dataset visualization
fig, axes = plt.subplots(2, 5, figsize=(10, 5))
axes = axes.flatten()
 
for i, ax in enumerate(axes):
    ax.imshow(X_train[i], cmap='gray')
    ax.axis('off')
    ax.set_title(class_names[y_train[i]])
 
plt.tight_layout()
plt.show()
 
# Preprocess the data
X_train = X_train.reshape(-1, 28 * 28) / 255.0
X_test = X_test.reshape(-1, 28 * 28) / 255.0
 
# Create the neural network model
model = Sequential()
model.add(Dense(256, activation='relu', input_shape=(28 * 28,)))
model.add(Dense(128, activation='relu'))
model.add(Dense(10, activation='softmax'))
model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])
model.summary()
 
# Train the model
history = model.fit(X_train, y_train, epochs=10, batch_size=32, validation_split=0.1)
 
# Evaluate the model
loss, accuracy = model.evaluate(X_test, y_test)
print("Test Loss:", loss)
print("Test Accuracy:", accuracy)
 
# Plot the training and validation loss
plt.figure(figsize=(8, 5))
plt.plot(history.history['loss'], label='Training Loss')
plt.plot(history.history['val_loss'], label='Validation Loss')
plt.xlabel('Epoch')
plt.ylabel('Loss')
plt.title('Training and Validation Loss')
plt.legend()
plt.show()
 
# Plot the training and validation accuracy
plt.figure(figsize=(8, 5))
plt.plot(history.history['accuracy'], label='Training Accuracy')
plt.plot(history.history['val_accuracy'], label='Validation Accuracy')
plt.xlabel('Epoch')
plt.ylabel('Accuracy')
plt.title('Training and Validation Accuracy')
plt.legend()
plt.show()
 
# Make predictions on the test set
y_pred = model.predict(X_test)
 
# Calculate the accuracy of the model
score = model.evaluate(X_test,y_test,verbose=0)
print("Test Loss", score[0])
print("Test Accuracy", score[1])
 
 